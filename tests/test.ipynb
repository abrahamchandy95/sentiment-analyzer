{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "124c4213-fb6b-4dbf-ad56-572113ffe948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from skimage.io import imread\n",
    "from skimage.color import gray2rgb\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "class ImageVectorizer:\n",
    "    def __init__(self, dims: Tuple[int, int]):\n",
    "        self.dims = dims\n",
    "        self.num_channels = 3\n",
    "\n",
    "    def convert_image_to_array(self, image_path: str) -> np.ndarray:\n",
    "        image_array = imread(image_path)\n",
    "        if len(image_array.shape) == 2:  # grayscale image\n",
    "            image_array = gray2rgb(image_array)\n",
    "        elif image_array.shape[2] > self.num_channels:\n",
    "            image_array = image_array[:, :, : self.num_channels]\n",
    "        image_array = resize(\n",
    "            image_array, (self.dims[0], self.dims[1]),\n",
    "            anti_aliasing=True\n",
    "        )\n",
    "        return image_array\n",
    "\n",
    "    def extract_features_from_images(\n",
    "        self, image_paths: List[str]\n",
    "    ) -> np.ndarray:\n",
    "        num_images = len(image_paths)\n",
    "        image_features = np.zeros(\n",
    "            (num_images, self.dims[0], self.dims[1], self.num_channels),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        for idx, img_path in enumerate(image_paths):\n",
    "            image_features[idx] = self.convert_image_to_array(img_path)\n",
    "        return image_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "585832a6-817b-4eb4-bf67-7766201daf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT_DIR = \"/Users/abraham/Documents/Work/Self/2024/sentiment_ai\"\n",
    "datasets_dir = os.path.join(PROJECT_DIR, \"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ddb54a-a07f-4ff2-ac7b-05aa0d50f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "fer_train_dir = os.path.join(datasets_dir, 'FER2013', 'train')\n",
    "fer_test_dir = os.path.join(datasets_dir, 'FER2013', 'test')\n",
    "\n",
    "def collect_labeled_images_from_dir(dir_path):\n",
    "    # Directory_sentiment map\n",
    "    dir_sentiment_map = {\n",
    "        'happy': 0,\n",
    "        'sad': 1,\n",
    "        'fear': 2,\n",
    "        'surprise': 3,\n",
    "        'neutral': 4,\n",
    "        'angry': 5,\n",
    "        'disgust': 6,\n",
    "    }\n",
    "    labeled_images = {}\n",
    "    for sentiment, label in dir_sentiment_map.items():\n",
    "        sentiment_img_paths = os.path.join(dir_path, sentiment)\n",
    "        for img_path in glob(os.path.join(sentiment_img_paths, '*jpg')):\n",
    "            labeled_images[img_path] = label\n",
    "    return labeled_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47f03229-f0c1-45f4-bae9-ffefebaaf342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "dims = (256, 256)\n",
    "img_vectortizer = ImageVectorizer(dims=dims)\n",
    "\n",
    "def extract_features_and_labels(\n",
    "    labeled_images: Dict[str, int], vectorizer: ImageVectorizer\n",
    ")-> Tuple[np.ndarray, np.ndarray]:\n",
    "    image_paths = list(labeled_images.keys())\n",
    "    labels = list(labeled_images.values())\n",
    "    # Extract features\n",
    "    image_features = vectorizer.extract_features_from_images(image_paths)\n",
    "    one_hot_labels = utils.to_categorical(labels, num_classes=len(set(labels)))\n",
    "    return image_features, one_hot_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef82f860-44e0-4c71-910f-1f70ac25878b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import utils\n",
    "dims = (256, 256)\n",
    "img_vectorizer = ImageVectorizer(dims=dims)\n",
    "train_images = collect_labeled_images_from_dir(fer_train_dir)\n",
    "X_train, y_train = extract_features_and_labels(\n",
    "    train_images, img_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad443d38-a35d-495c-ab1d-4212cd6bc05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = collect_labeled_images_from_dir(fer_test_dir)\n",
    "X_test, y_test = extract_features_and_labels(test_images, img_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4193310-9b9b-4e77-b8e2-59029997e4dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 256, 256, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fbe2616-c119-48ea-b8a0-a83578a9a15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d65ef83-dc12-4389-9c00-9d969ffb124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers, callbacks\n",
    "\n",
    "class FaceSentimentCNNModel:\n",
    "\n",
    "    def __init__(self, image_shape, num_sentiments):\n",
    "        self.image_shape = image_shape\n",
    "        self.num_sentiments = num_sentiments\n",
    "        self.model = self._build_model()\n",
    "        self._compile_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        model = models.Sequential([\n",
    "            layers.Input(shape=self.image_shape),\n",
    "            \n",
    "            layers.Conv2D(\n",
    "                32, kernel_size=(3, 3), padding='same', \n",
    "                kernel_initializer='glorot_uniform', bias_initializer='zeros'\n",
    "            ),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation('relu'),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "            layers.Dropout(0.25),\n",
    "            \n",
    "            layers.Conv2D(\n",
    "                64, kernel_size=(3, 3), padding='same', \n",
    "                kernel_initializer='glorot_uniform', bias_initializer='zeros'\n",
    "            ),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation('relu'),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "            layers.Dropout(0.25),\n",
    "            \n",
    "            layers.Conv2D(\n",
    "                128, kernel_size=(3, 3), padding='same', \n",
    "                kernel_initializer='glorot_uniform', bias_initializer='zeros'\n",
    "            ),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation('relu'),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "            layers.Dropout(0.25),\n",
    "            \n",
    "            layers.Conv2D(\n",
    "                256, kernel_size=(3, 3), padding='same', \n",
    "                kernel_initializer='glorot_uniform', bias_initializer='zeros'\n",
    "            ),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation('relu'),\n",
    "            \n",
    "            layers.Conv2D(\n",
    "                256, kernel_size=(3, 3), padding='same', \n",
    "                kernel_initializer='glorot_uniform', bias_initializer='zeros'\n",
    "            ),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation('relu'),\n",
    "            layers.AveragePooling2D(pool_size=(2, 2)),\n",
    "            layers.Dropout(0.5),\n",
    "            \n",
    "            layers.Flatten(),\n",
    "            \n",
    "            layers.Dense(\n",
    "                512, kernel_initializer='glorot_uniform', \n",
    "                bias_initializer='zeros'\n",
    "            ),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Activation('relu'),\n",
    "            layers.Dropout(0.5),\n",
    "            \n",
    "            layers.Dense(\n",
    "                self.num_sentiments, kernel_initializer='glorot_uniform', \n",
    "                bias_initializer='zeros'\n",
    "            ),\n",
    "            layers.Activation('softmax')\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    def _compile_model(self):\n",
    "            fer_optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "            self.model.compile(\n",
    "                optimizer=fer_optimizer, loss='categorical_crossentropy', \n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2239ff9f-77fb-4f41-be87-4469b345e52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (dims[0], dims[1], 3)\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "num_sentiments=7\n",
    "cnn_model = FaceSentimentCNNModel(image_shape, num_sentiments)\n",
    "model = cnn_model.get_model()\n",
    "checkpoint_dir = os.path.join(PROJECT_DIR, 'models')\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "checkpoint = callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(checkpoint_dir, 'face_modelCNN.keras'),\n",
    "    monitor='val_loss', save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc9ed48-9053-4c93-9c51-6772f804760b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.2706 - loss: 2.0134\n",
      "Epoch 1: val_loss improved from inf to 1.90018, saving model to /Users/abraham/Documents/Work/Self/2024/sentiment_ai/models/face_modelCNN.keras\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1427s\u001b[0m 2s/step - accuracy: 0.2706 - loss: 2.0132 - val_accuracy: 0.2106 - val_loss: 1.9002\n",
      "Epoch 2/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4279 - loss: 1.4889\n",
      "Epoch 2: val_loss improved from 1.90018 to 1.40048, saving model to /Users/abraham/Documents/Work/Self/2024/sentiment_ai/models/face_modelCNN.keras\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1395s\u001b[0m 2s/step - accuracy: 0.4279 - loss: 1.4888 - val_accuracy: 0.4570 - val_loss: 1.4005\n",
      "Epoch 3/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5082 - loss: 1.2918\n",
      "Epoch 3: val_loss did not improve from 1.40048\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1381s\u001b[0m 2s/step - accuracy: 0.5082 - loss: 1.2918 - val_accuracy: 0.4210 - val_loss: 1.5630\n",
      "Epoch 4/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5588 - loss: 1.1610\n",
      "Epoch 4: val_loss improved from 1.40048 to 1.29232, saving model to /Users/abraham/Documents/Work/Self/2024/sentiment_ai/models/face_modelCNN.keras\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1384s\u001b[0m 2s/step - accuracy: 0.5588 - loss: 1.1610 - val_accuracy: 0.5063 - val_loss: 1.2923\n",
      "Epoch 5/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6016 - loss: 1.0608\n",
      "Epoch 5: val_loss improved from 1.29232 to 1.21484, saving model to /Users/abraham/Documents/Work/Self/2024/sentiment_ai/models/face_modelCNN.keras\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1114s\u001b[0m 1s/step - accuracy: 0.6016 - loss: 1.0608 - val_accuracy: 0.5411 - val_loss: 1.2148\n",
      "Epoch 6/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6414 - loss: 0.9688\n",
      "Epoch 6: val_loss improved from 1.21484 to 1.17599, saving model to /Users/abraham/Documents/Work/Self/2024/sentiment_ai/models/face_modelCNN.keras\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m955s\u001b[0m 1s/step - accuracy: 0.6414 - loss: 0.9688 - val_accuracy: 0.5563 - val_loss: 1.1760\n",
      "Epoch 7/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998ms/step - accuracy: 0.6829 - loss: 0.8634\n",
      "Epoch 7: val_loss did not improve from 1.17599\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m948s\u001b[0m 1s/step - accuracy: 0.6829 - loss: 0.8634 - val_accuracy: 0.5541 - val_loss: 1.2081\n",
      "Epoch 8/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7248 - loss: 0.7609\n",
      "Epoch 8: val_loss did not improve from 1.17599\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m952s\u001b[0m 1s/step - accuracy: 0.7248 - loss: 0.7609 - val_accuracy: 0.5797 - val_loss: 1.2042\n",
      "Epoch 9/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7575 - loss: 0.6684\n",
      "Epoch 9: val_loss improved from 1.17599 to 1.17189, saving model to /Users/abraham/Documents/Work/Self/2024/sentiment_ai/models/face_modelCNN.keras\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1046s\u001b[0m 1s/step - accuracy: 0.7575 - loss: 0.6684 - val_accuracy: 0.5865 - val_loss: 1.1719\n",
      "Epoch 10/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7907 - loss: 0.5848\n",
      "Epoch 10: val_loss did not improve from 1.17189\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m970s\u001b[0m 1s/step - accuracy: 0.7907 - loss: 0.5848 - val_accuracy: 0.5805 - val_loss: 1.2810\n",
      "Epoch 11/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.8273 - loss: 0.4981\n",
      "Epoch 11: val_loss did not improve from 1.17189\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5424s\u001b[0m 6s/step - accuracy: 0.8273 - loss: 0.4981 - val_accuracy: 0.5879 - val_loss: 1.2553\n",
      "Epoch 12/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15s/step - accuracy: 0.8511 - loss: 0.4283 \n",
      "Epoch 12: val_loss did not improve from 1.17189\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14447s\u001b[0m 16s/step - accuracy: 0.8510 - loss: 0.4283 - val_accuracy: 0.6085 - val_loss: 1.2488\n",
      "Epoch 13/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.8690 - loss: 0.3688\n",
      "Epoch 13: val_loss did not improve from 1.17189\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5028s\u001b[0m 6s/step - accuracy: 0.8690 - loss: 0.3688 - val_accuracy: 0.5614 - val_loss: 1.3286\n",
      "Epoch 14/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 974ms/step - accuracy: 0.8818 - loss: 0.3382\n",
      "Epoch 14: val_loss did not improve from 1.17189\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m924s\u001b[0m 1s/step - accuracy: 0.8817 - loss: 0.3382 - val_accuracy: 0.6066 - val_loss: 1.2716\n",
      "Epoch 15/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981ms/step - accuracy: 0.8993 - loss: 0.2988\n",
      "Epoch 15: val_loss did not improve from 1.17189\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m933s\u001b[0m 1s/step - accuracy: 0.8993 - loss: 0.2988 - val_accuracy: 0.5903 - val_loss: 1.4933\n",
      "Epoch 16/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9110 - loss: 0.2597\n",
      "Epoch 16: val_loss did not improve from 1.17189\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m953s\u001b[0m 1s/step - accuracy: 0.9110 - loss: 0.2597 - val_accuracy: 0.6017 - val_loss: 1.4384\n",
      "Epoch 17/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9171 - loss: 0.2425\n",
      "Epoch 17: val_loss did not improve from 1.17189\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15445s\u001b[0m 17s/step - accuracy: 0.9171 - loss: 0.2425 - val_accuracy: 0.5981 - val_loss: 1.4790\n",
      "Epoch 18/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9274 - loss: 0.2117\n",
      "Epoch 18: val_loss did not improve from 1.17189\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m982s\u001b[0m 1s/step - accuracy: 0.9274 - loss: 0.2118 - val_accuracy: 0.6067 - val_loss: 1.4777\n",
      "Epoch 19/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9278 - loss: 0.2086\n",
      "Epoch 19: val_loss did not improve from 1.17189\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1072s\u001b[0m 1s/step - accuracy: 0.9278 - loss: 0.2086 - val_accuracy: 0.5634 - val_loss: 1.6063\n",
      "Epoch 20/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9288 - loss: 0.2052\n",
      "Epoch 20: val_loss did not improve from 1.17189\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1112s\u001b[0m 1s/step - accuracy: 0.9288 - loss: 0.2052 - val_accuracy: 0.6046 - val_loss: 1.6129\n",
      "Epoch 21/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9348 - loss: 0.1935\n",
      "Epoch 21: val_loss did not improve from 1.17189\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1065s\u001b[0m 1s/step - accuracy: 0.9347 - loss: 0.1935 - val_accuracy: 0.6028 - val_loss: 1.4532\n",
      "Epoch 22/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9421 - loss: 0.1738\n",
      "Epoch 22: val_loss did not improve from 1.17189\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m990s\u001b[0m 1s/step - accuracy: 0.9421 - loss: 0.1738 - val_accuracy: 0.6028 - val_loss: 1.4727\n",
      "Epoch 23/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9446 - loss: 0.1612\n",
      "Epoch 23: val_loss did not improve from 1.17189\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1055s\u001b[0m 1s/step - accuracy: 0.9446 - loss: 0.1612 - val_accuracy: 0.6073 - val_loss: 1.6744\n",
      "Epoch 24/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9477 - loss: 0.1515\n",
      "Epoch 24: val_loss did not improve from 1.17189\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1116s\u001b[0m 1s/step - accuracy: 0.9477 - loss: 0.1515 - val_accuracy: 0.6113 - val_loss: 1.4960\n",
      "Epoch 25/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9481 - loss: 0.1546\n",
      "Epoch 25: val_loss did not improve from 1.17189\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2351s\u001b[0m 3s/step - accuracy: 0.9481 - loss: 0.1546 - val_accuracy: 0.6133 - val_loss: 1.7883\n",
      "Epoch 26/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.9537 - loss: 0.1421\n",
      "Epoch 26: val_loss did not improve from 1.17189\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7236s\u001b[0m 8s/step - accuracy: 0.9537 - loss: 0.1421 - val_accuracy: 0.6112 - val_loss: 1.7931\n",
      "Epoch 27/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9568 - loss: 0.1341\n",
      "Epoch 27: val_loss did not improve from 1.17189\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1207s\u001b[0m 1s/step - accuracy: 0.9568 - loss: 0.1341 - val_accuracy: 0.6089 - val_loss: 1.8096\n",
      "Epoch 28/30\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9549 - loss: 0.1408\n",
      "Epoch 28: val_loss did not improve from 1.17189\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m986s\u001b[0m 1s/step - accuracy: 0.9549 - loss: 0.1408 - val_accuracy: 0.6115 - val_loss: 1.7950\n",
      "Epoch 29/30\n",
      "\u001b[1m726/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m3:33\u001b[0m 1s/step - accuracy: 0.9595 - loss: 0.1291"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, y_train, batch_size=batch_size, epochs=epochs, \n",
    "    validation_data=(X_test, y_test), shuffle=True, callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c903e521-65ce-473b-a2f9-e8e496afac80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
